{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#prepare data \n",
    "#%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from datasets import TripletAudio\n",
    "\n",
    "\n",
    "K, MAX_CLOSE_NEG, MAX_FAR_NEG, P_STRONG_NEG = 5, 15, 15, 0.9\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "triplet_train_dataset = TripletAudio(True, K, MAX_CLOSE_NEG, MAX_FAR_NEG, P_STRONG_NEG)\n",
    "triplet_test_dataset = TripletAudio(False, K, MAX_CLOSE_NEG, MAX_FAR_NEG, P_STRONG_NEG)\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising model biases\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Set up the network and training parameters\n",
    "from networks import AnchorNet, EmbeddingNet, TripletNet\n",
    "from losses import TripletLoss\n",
    "import torch.optim as optim\n",
    "from recall import Recall\n",
    "from torch.optim import lr_scheduler\n",
    "from recall import Recall\n",
    "from trainer import fit\n",
    "\n",
    "INPUT_D, OUTPUT_D = 192, 128\n",
    "MARGIN, LEARNING_RATE, N_EPOCHS, LOG_INT, N_RECALL_CAND = 0.5, 1e-3, 10, 100, 25\n",
    "\n",
    "#define model\n",
    "anchor_net = AnchorNet(triplet_train_dataset.get_dataset(), INPUT_D, OUTPUT_D)\n",
    "embedding_net = EmbeddingNet(anchor_net)\n",
    "model = TripletNet(embedding_net)\n",
    "\n",
    "loss_fn = TripletLoss(MARGIN)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1, last_epoch=-1)\n",
    "\n",
    "recall = Recall(N_RECALL_CAND, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/48048 (0%)]\tLoss: 17.467911\n",
      "Train: [12800/48048 (27%)]\tLoss: 17.202353\n",
      "Train: [25600/48048 (53%)]\tLoss: 16.524647\n",
      "Train: [38400/48048 (80%)]\tLoss: 13.642067\n",
      "Epoch: 1/10. Train set: Average loss: 14.9202\n",
      "Epoch: 1/10. Validation set: Average loss: 17.9979\n",
      "Train: [0/48048 (0%)]\tLoss: 10.672917\n",
      "Train: [12800/48048 (27%)]\tLoss: 9.749786\n",
      "Train: [25600/48048 (53%)]\tLoss: 8.314637\n",
      "Train: [38400/48048 (80%)]\tLoss: 6.884268\n",
      "Epoch: 2/10. Train set: Average loss: 7.9613\n",
      "Epoch: 2/10. Validation set: Average loss: 9.2268\n",
      "Train: [0/48048 (0%)]\tLoss: 11.512088\n",
      "Train: [12800/48048 (27%)]\tLoss: 5.345594\n",
      "Train: [25600/48048 (53%)]\tLoss: 4.759333\n",
      "Train: [38400/48048 (80%)]\tLoss: 4.265430\n",
      "Epoch: 3/10. Train set: Average loss: 4.6526\n",
      "Epoch: 3/10. Validation set: Average loss: 5.5178\n",
      "Train: [0/48048 (0%)]\tLoss: 2.135470\n",
      "Train: [12800/48048 (27%)]\tLoss: 3.616093\n",
      "Train: [25600/48048 (53%)]\tLoss: 3.205626\n",
      "Train: [38400/48048 (80%)]\tLoss: 3.036918\n",
      "Epoch: 4/10. Train set: Average loss: 3.2106\n",
      "Epoch: 4/10. Validation set: Average loss: 3.7023\n",
      "Train: [0/48048 (0%)]\tLoss: 2.514069\n",
      "Train: [12800/48048 (27%)]\tLoss: 2.350341\n",
      "Train: [25600/48048 (53%)]\tLoss: 2.396987\n",
      "Train: [38400/48048 (80%)]\tLoss: 2.338220\n",
      "Epoch: 5/10. Train set: Average loss: 2.3511\n",
      "Epoch: 5/10. Validation set: Average loss: 2.8918\n",
      "Train: [0/48048 (0%)]\tLoss: 4.335223\n",
      "Train: [12800/48048 (27%)]\tLoss: 2.222801\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.893264\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.907362\n",
      "Epoch: 6/10. Train set: Average loss: 1.9684\n",
      "Epoch: 6/10. Validation set: Average loss: 2.3934\n",
      "Train: [0/48048 (0%)]\tLoss: 1.542296\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.811893\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.689452\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.444656\n",
      "Epoch: 7/10. Train set: Average loss: 1.6270\n",
      "Epoch: 7/10. Validation set: Average loss: 2.0061\n",
      "Train: [0/48048 (0%)]\tLoss: 1.567425\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.511821\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.421720\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.415016\n",
      "Epoch: 8/10. Train set: Average loss: 1.4176\n",
      "Epoch: 8/10. Validation set: Average loss: 1.7758\n",
      "Train: [0/48048 (0%)]\tLoss: 1.422967\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.406597\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.230419\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.360347\n",
      "Epoch: 9/10. Train set: Average loss: 1.2998\n",
      "Epoch: 9/10. Validation set: Average loss: 1.5654\n",
      "Train: [0/48048 (0%)]\tLoss: 1.046553\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.184333\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.153016\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.151692\n",
      "Epoch: 10/10. Train set: Average loss: 1.1474\n",
      "Epoch: 10/10. Validation set: Average loss: 1.3609\n"
     ]
    }
   ],
   "source": [
    "#run the model\n",
    "train_loss, val_loss = fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, N_EPOCHS, {}, LOG_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# perform recall on random batch\n",
    "outputs = []\n",
    "for d in (triplet_train_dataset, triplet_test_dataset):\n",
    "    batch_indicies = np.random.choice(list(range(0, d.get_dataset().shape[0])), N_RECALL_SAMPLES, False)\n",
    "    queries = d.get_dataset()[batch_indicies]\n",
    "    true_knns = d.get_knn().iloc[batch_indicies, :]\n",
    "    outputs.append(recall.calculate(d.get_dataset(), model.embedding_net, queries, true_knns, False))\n",
    "train_recall, val_recall = np.mean(outputs[0]), np.mean(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter() as w:\n",
    "    w.add_hparams(\n",
    "        {'LR': LEARNING_RATE, 'BSIZE': BATCH_SIZE, 'N_RECALL_S': N_RECALL_SAMPLES, 'LEARNING RATE': LEARNING_RATE, 'STEP_SIZE': STEP_SIZE},\n",
    "        {'TRAIN_L': train_loss, 'VAL_L': val_loss, 'TRAIN_RECALL': train_recall, 'VAL_RECALL': val_recall})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import torch\n",
    "from datasets import TripletAudio\n",
    "from networks import AnchorNet, EmbeddingNet, TripletNet\n",
    "from losses import TripletLoss\n",
    "import torch.optim as optim\n",
    "from recall import Recall\n",
    "from torch.optim import lr_scheduler\n",
    "from trainer import fit\n",
    "from recall import Recall\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparams\n",
    "K, MAX_CLOSE_NEG, MAX_FAR_NEG, P_STRONG_NEG = 5, 5, 0, 1\n",
    "BATCH_SIZE = 128\n",
    "INPUT_D, OUTPUT_D = 192, 128\n",
    "MARGIN, LEARNING_RATE, N_EPOCHS, LOG_INT, N_RECALL_CAND = 0.5, 1e-3, 25, 100, 25\n",
    "N_RECALL_SAMPLES = 1000\n",
    "# hyperparams = [MAX_CLOSE_NEGS, MAX_FAR_NEGS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes the recall on datasets using global variables such as model and recall\n",
    "def measure_recall(datasets):\n",
    "    outputs = []\n",
    "    for d in datasets:\n",
    "        batch_indicies = np.random.choice(list(range(0, d.get_dataset().shape[0])), N_RECALL_SAMPLES, False)\n",
    "        queries = d.get_dataset()[batch_indicies]\n",
    "        true_knns = d.get_knn().iloc[batch_indicies, :]\n",
    "        outputs.append(recall.calculate(d.get_dataset(), model.embedding_net, queries, true_knns, False))\n",
    "    return [np.mean(x) for x in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising model biases\n",
      "done\n",
      "Train: [0/48048 (0%)]\tLoss: 29.048782\n",
      "index 38857, pos 44064, neg 14605\n",
      "Train: [12800/48048 (27%)]\tLoss: 20.401318\n",
      "Train: [25600/48048 (53%)]\tLoss: 20.158985\n",
      "Train: [38400/48048 (80%)]\tLoss: 18.834817\n",
      "Epoch: 1/25. Train set: Average loss: 19.4634\n",
      "Epoch: 1/25. Validation set: Average loss: 17.9263\n",
      "Train: [0/48048 (0%)]\tLoss: 12.652967\n",
      "Train: [12800/48048 (27%)]\tLoss: 16.488730\n",
      "Train: [25600/48048 (53%)]\tLoss: 15.225643\n",
      "Train: [38400/48048 (80%)]\tLoss: 13.848978\n",
      "index 38857, pos 44064, neg 37043\n",
      "Epoch: 2/25. Train set: Average loss: 14.3974\n",
      "Epoch: 2/25. Validation set: Average loss: 12.2914\n",
      "Train: [0/48048 (0%)]\tLoss: 12.060904\n",
      "Train: [12800/48048 (27%)]\tLoss: 11.113049\n",
      "Train: [25600/48048 (53%)]\tLoss: 10.311789\n",
      "Train: [38400/48048 (80%)]\tLoss: 8.881455\n",
      "index 38857, pos 44064, neg 37043\n",
      "Epoch: 3/25. Train set: Average loss: 9.6451\n",
      "Epoch: 3/25. Validation set: Average loss: 7.8716\n",
      "Train: [0/48048 (0%)]\tLoss: 9.496128\n",
      "index 38857, pos 44064, neg 37043\n",
      "Train: [12800/48048 (27%)]\tLoss: 7.244343\n",
      "Train: [25600/48048 (53%)]\tLoss: 6.210404\n",
      "Train: [38400/48048 (80%)]\tLoss: 6.026899\n",
      "Epoch: 4/25. Train set: Average loss: 6.3651\n",
      "Epoch: 4/25. Validation set: Average loss: 5.5207\n",
      "Train: [0/48048 (0%)]\tLoss: 4.369833\n",
      "Train: [12800/48048 (27%)]\tLoss: 5.131472\n",
      "index 38857, pos 44064, neg 20483\n",
      "Train: [25600/48048 (53%)]\tLoss: 4.925816\n",
      "Train: [38400/48048 (80%)]\tLoss: 4.493769\n",
      "Epoch: 5/25. Train set: Average loss: 4.7106\n",
      "Epoch: 5/25. Validation set: Average loss: 4.1835\n",
      "Train: [0/48048 (0%)]\tLoss: 5.820810\n",
      "Train: [12800/48048 (27%)]\tLoss: 3.976418\n",
      "Train: [25600/48048 (53%)]\tLoss: 3.852122\n",
      "index 38857, pos 44064, neg 33278\n",
      "Train: [38400/48048 (80%)]\tLoss: 3.602613\n",
      "Epoch: 6/25. Train set: Average loss: 3.7087\n",
      "Epoch: 6/25. Validation set: Average loss: 3.2636\n",
      "Train: [0/48048 (0%)]\tLoss: 2.677583\n",
      "index 38857, pos 44064, neg 29456\n",
      "Train: [12800/48048 (27%)]\tLoss: 3.306475\n",
      "Train: [25600/48048 (53%)]\tLoss: 2.678420\n",
      "Train: [38400/48048 (80%)]\tLoss: 2.896393\n",
      "Epoch: 7/25. Train set: Average loss: 2.9282\n",
      "Epoch: 7/25. Validation set: Average loss: 2.6450\n",
      "Train: [0/48048 (0%)]\tLoss: 1.297091\n",
      "index 38857, pos 44064, neg 29456\n",
      "Train: [12800/48048 (27%)]\tLoss: 2.507767\n",
      "Train: [25600/48048 (53%)]\tLoss: 2.421845\n",
      "Train: [38400/48048 (80%)]\tLoss: 2.480391\n",
      "Epoch: 8/25. Train set: Average loss: 2.4170\n",
      "Epoch: 8/25. Validation set: Average loss: 2.3336\n",
      "Train: [0/48048 (0%)]\tLoss: 2.626074\n",
      "Train: [12800/48048 (27%)]\tLoss: 2.419524\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.999216\n",
      "index 38857, pos 44064, neg 29456\n",
      "Train: [38400/48048 (80%)]\tLoss: 2.070043\n",
      "Epoch: 9/25. Train set: Average loss: 2.1733\n",
      "Epoch: 9/25. Validation set: Average loss: 2.1217\n",
      "Train: [0/48048 (0%)]\tLoss: 3.543576\n",
      "Train: [12800/48048 (27%)]\tLoss: 2.037210\n",
      "index 38857, pos 44064, neg 37043\n",
      "Train: [25600/48048 (53%)]\tLoss: 2.112757\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.934809\n",
      "Epoch: 10/25. Train set: Average loss: 1.9839\n",
      "Epoch: 10/25. Validation set: Average loss: 1.9705\n",
      "Train: [0/48048 (0%)]\tLoss: 1.096017\n",
      "index 38857, pos 44064, neg 33278\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.755472\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.739822\n",
      "Train: [38400/48048 (80%)]\tLoss: 2.025054\n",
      "Epoch: 11/25. Train set: Average loss: 1.8098\n",
      "Epoch: 11/25. Validation set: Average loss: 1.8711\n",
      "index 38857, pos 44064, neg 33278\n",
      "Train: [0/48048 (0%)]\tLoss: 1.755617\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.732233\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.610558\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.583274\n",
      "Epoch: 12/25. Train set: Average loss: 1.6103\n",
      "Epoch: 12/25. Validation set: Average loss: 1.7967\n",
      "Train: [0/48048 (0%)]\tLoss: 1.412857\n",
      "index 38857, pos 44064, neg 14605\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.516260\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.638706\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.529756\n",
      "Epoch: 13/25. Train set: Average loss: 1.5627\n",
      "Epoch: 13/25. Validation set: Average loss: 1.7274\n",
      "Train: [0/48048 (0%)]\tLoss: 0.770533\n",
      "index 38857, pos 44064, neg 14605\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.476426\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.470196\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.431309\n",
      "Epoch: 14/25. Train set: Average loss: 1.4757\n",
      "Epoch: 14/25. Validation set: Average loss: 1.6631\n",
      "Train: [0/48048 (0%)]\tLoss: 0.629020\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.388316\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.416530\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.341205\n",
      "index 38857, pos 44064, neg 37043\n",
      "Epoch: 15/25. Train set: Average loss: 1.3841\n",
      "Epoch: 15/25. Validation set: Average loss: 1.5976\n",
      "Train: [0/48048 (0%)]\tLoss: 1.735576\n",
      "index 38857, pos 44064, neg 14605\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.414039\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.283231\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.440055\n",
      "Epoch: 16/25. Train set: Average loss: 1.3549\n",
      "Epoch: 16/25. Validation set: Average loss: 1.5281\n",
      "Train: [0/48048 (0%)]\tLoss: 0.689344\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.234277\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.353662\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.283286\n",
      "index 38857, pos 44064, neg 29456\n",
      "Epoch: 17/25. Train set: Average loss: 1.2919\n",
      "Epoch: 17/25. Validation set: Average loss: 1.4578\n",
      "Train: [0/48048 (0%)]\tLoss: 6.104140\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.251713\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.180899\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.248223\n",
      "index 38857, pos 44064, neg 14605\n",
      "Epoch: 18/25. Train set: Average loss: 1.2506\n",
      "Epoch: 18/25. Validation set: Average loss: 1.4036\n",
      "Train: [0/48048 (0%)]\tLoss: 2.788760\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.207630\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.061386\n",
      "index 38857, pos 44064, neg 37043\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.289222\n",
      "Epoch: 19/25. Train set: Average loss: 1.2044\n",
      "Epoch: 19/25. Validation set: Average loss: 1.3638\n",
      "Train: [0/48048 (0%)]\tLoss: 2.170608\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.094545\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.136401\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.173357\n",
      "index 38857, pos 44064, neg 20483\n",
      "Epoch: 20/25. Train set: Average loss: 1.1732\n",
      "Epoch: 20/25. Validation set: Average loss: 1.3238\n",
      "Train: [0/48048 (0%)]\tLoss: 1.387238\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.115867\n",
      "index 38857, pos 44064, neg 29456\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.124208\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.286285\n",
      "Epoch: 21/25. Train set: Average loss: 1.1672\n",
      "Epoch: 21/25. Validation set: Average loss: 1.2881\n",
      "Train: [0/48048 (0%)]\tLoss: 1.770812\n",
      "index 38857, pos 44064, neg 33278\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.103539\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.180255\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.022847\n",
      "Epoch: 22/25. Train set: Average loss: 1.1024\n",
      "Epoch: 22/25. Validation set: Average loss: 1.2567\n",
      "Train: [0/48048 (0%)]\tLoss: 0.531519\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.199667\n",
      "index 38857, pos 44064, neg 37043\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.088455\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.019867\n",
      "Epoch: 23/25. Train set: Average loss: 1.0936\n",
      "Epoch: 23/25. Validation set: Average loss: 1.2297\n",
      "Train: [0/48048 (0%)]\tLoss: 2.026087\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.025364\n",
      "index 38857, pos 44064, neg 33278\n",
      "Train: [25600/48048 (53%)]\tLoss: 0.991150\n",
      "Train: [38400/48048 (80%)]\tLoss: 1.017973\n",
      "Epoch: 24/25. Train set: Average loss: 1.0422\n",
      "Epoch: 24/25. Validation set: Average loss: 1.2095\n",
      "Train: [0/48048 (0%)]\tLoss: 0.552877\n",
      "Train: [12800/48048 (27%)]\tLoss: 1.041507\n",
      "index 38857, pos 44064, neg 29456\n",
      "Train: [25600/48048 (53%)]\tLoss: 1.145807\n",
      "Train: [38400/48048 (80%)]\tLoss: 0.991028\n",
      "Epoch: 25/25. Train set: Average loss: 1.0376\n",
      "Epoch: 25/25. Validation set: Average loss: 1.1872\n",
      "embedding dataset\n",
      "quantizing dataset\n",
      "done quantizing into 399 buckets\n",
      "embedding dataset\n",
      "quantizing dataset\n",
      "done quantizing into 87 buckets\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SummaryWriter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-590176b7bc0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtriplet_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriplet_test_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#write to tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         w.add_hparams(\n\u001b[1;32m     22\u001b[0m             {'LR': LEARNING_RATE, 'BSIZE': BATCH_SIZE, 'N_RECALL_S': N_RECALL_SAMPLES, 'N_RECALL_CAND': N_RECALL_CAND,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SummaryWriter' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,1): #itertools.product(*hyperparams):\n",
    "    #setup datasets\n",
    "    triplet_train_dataset = TripletAudio(True, K, MAX_CLOSE_NEG, MAX_FAR_NEG, P_STRONG_NEG)\n",
    "    triplet_test_dataset = TripletAudio(False, K, MAX_CLOSE_NEG, MAX_FAR_NEG, P_STRONG_NEG)\n",
    "    triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    #define model \n",
    "    anchor_net = AnchorNet(triplet_train_dataset.get_dataset(), INPUT_D, OUTPUT_D)\n",
    "    embedding_net = EmbeddingNet(anchor_net)\n",
    "    model = TripletNet(embedding_net)\n",
    "    loss_fn = TripletLoss(MARGIN)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1, last_epoch=-1)\n",
    "    recall = Recall(N_RECALL_CAND, K)\n",
    "    #run the model\n",
    "    train_loss, val_loss = fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, N_EPOCHS, {}, LOG_INT)\n",
    "    #measure recall\n",
    "    train_recall, val_recall = measure_recall([triplet_train_dataset, triplet_test_dataset])\n",
    "    #write to tensorboard\n",
    "    with SummaryWriter() as w:\n",
    "        w.add_hparams(\n",
    "            {'LR': LEARNING_RATE, 'BSIZE': BATCH_SIZE, 'N_RECALL_S': N_RECALL_SAMPLES, 'N_RECALL_CAND': N_RECALL_CAND,\n",
    "                 'CLOSE_NEG': MAX_CLOSE_NEG, 'FAR_NEG': MAX_FAR_NEG, 'P_STRONG_NEG': P_STRONG_NEG, 'OUTPUT_D': OUTPUT_D},\n",
    "            {'TRAIN_L': train_loss, 'VAL_L': val_loss, 'TRAIN_RECALL': train_recall, 'VAL_RECALL': val_recall})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#prepare data \n",
    "# %load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from datasets import AudioTrainDataset, AudioTestDataset\n",
    "from datasets import BalancedBatchSampler\n",
    "\n",
    "K = 5\n",
    "train_dataset = AudioTrainDataset(K)\n",
    "test_dataset = AudioTestDataset(K)\n",
    "\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset)\n",
    "\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the network and training parameters\n",
    "from networks import EmbeddingNet, AnchorNet\n",
    "from losses import OnlineTripletLoss\n",
    "from utils import SemihardNegativeTripletSelector, HardestNegativeTripletSelector\n",
    "from metrics import AverageNonzeroTripletsMetric\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "INPUT_D, OUTPUT_D = 192, 128\n",
    "MARGIN, LEARNING_RATE, N_EPOCHS, LOG_INT = 0.5, 1e-3, 5, 50\n",
    "\n",
    "#define model\n",
    "anchor_net = AnchorNet(train_dataset.data, INPUT_D, OUTPUT_D)\n",
    "model = EmbeddingNet(anchor_net)\n",
    "loss_fn = OnlineTripletLoss(MARGIN, SemihardNegativeTripletSelector(MARGIN, train_dataset.KNN))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#run model\n",
    "from trainer import fit\n",
    "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, N_EPOCHS, {}, LOG_INT, metrics=[AverageNonzeroTripletsMetric()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis] *",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "415px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

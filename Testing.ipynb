{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks the KNN's rankings of datapoints is correct. i.e. anchor-i dist < anchor-j dist for i < j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    return (x-y).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-304-7200cd0fbda9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprev_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcur_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_dist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcur_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All Good!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import TripletAudio\n",
    "train_data = torch.from_numpy(np.loadtxt('data/trainData.txt', dtype=np.float32))\n",
    "train_KNN = pd.read_csv('data/trainKNN.csv', index_col=0)\n",
    "# check num elems are same in DF and dataset\n",
    "assert(train_data.shape[0] == train_KNN.shape[0])\n",
    "# #randomly pick 10 rows to check\n",
    "selected_indicies = np.random.choice(list(range(0,train_KNN.shape[0])), size=100, replace=False)\n",
    "# ensure that the ordering of neighbours is correct for that row\n",
    "for index in selected_indicies:\n",
    "    row = train_KNN.iloc[index]\n",
    "    anchor = train_data[index]\n",
    "    for prev, cur in zip(row, row[1:]):\n",
    "        prev_dist = dist(anchor, train_data[prev])\n",
    "        cur_dist = dist(anchor, train_data[cur])\n",
    "        assert(prev_dist < cur_dist)\n",
    "print(\"All Good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generates triplets and ensures that the anchor-pos distance < anchor-neg distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good!\n"
     ]
    }
   ],
   "source": [
    "from datasets import TripletAudio\n",
    "dataset = TripletAudio(True, 5, 5, 5)\n",
    "for i, ((anchor, pos, neg), label, index) in enumerate(dataset):\n",
    "    assert((np.linalg.norm(anchor-pos) - np.linalg.norm(anchor-neg)) < 0)\n",
    "print(\"All Good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runs two checks for the embedding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good!\n"
     ]
    }
   ],
   "source": [
    "from networks import AnchorNet\n",
    "#test batch size 1\n",
    "train_data = torch.tensor([[[2.],[3.]]])\n",
    "anchor = AnchorNet([], 2, 1, True)\n",
    "expected_output = torch.tensor([[5**(1/2) - 1]])\n",
    "output = anchor.forward(train_data)\n",
    "assert(torch.equal(output, expected_output))\n",
    "# test batch size 2\n",
    "train_data = torch.tensor([[[12.],[8.]],[[6.],[9.]]])\n",
    "anchor = AnchorNet(train_data, 2, 1, True)\n",
    "expected_output = torch.tensor([[170**(1/2) - 1], [89**(1/2) - 1]])\n",
    "output = anchor.forward(train_data)\n",
    "assert(torch.equal(output, expected_output))\n",
    "print(\"All Good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks similarity generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import similarity_calculator \n",
    "similarity = similarity_calculator(train_KNN, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indicies = np.random.choice(list(range(0,train_KNN.shape[0])), size=100, replace=False)\n",
    "for index in selected_indicies:\n",
    "    similarity_indicies = np.where(similarity[index])[0] #indicies from similarity matrix\n",
    "    expected_indicies = np.append(train_KNN.iloc[index][:5], index) #get true KNN's and add index itself\n",
    "    np.testing.assert_array_equal(np.sort(similarity_indicies), np.sort(expected_indicies))\n",
    "print(\"All Good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks the negative triplet selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mock df to use as the knn dataset\n",
    "mock_df = pd.DataFrame(columns=list(range(0,5)))\n",
    "mock_df.loc[0] = [1,2,5,6,7] #reciprocal neighbours with 1,2. One way with 3\n",
    "mock_df.loc[1] = [0,6,7,8,9] #reciprocal neighbours with 0\n",
    "mock_df.loc[2] = [0,10,11,12,13] #reciprocal neighbours with 0\n",
    "mock_df.loc[3] = [0,15,16,17,18] #one way with 0\n",
    "mock_df.loc[4] = [18,19,20,21,22] #no NN\n",
    "# need to mock remaining for similaritycalc\n",
    "for i in range(5,23):\n",
    "    mock_df.loc[i] = [18,19,20,21,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating similarity matrix\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from utils import SemihardNegativeTripletSelector\n",
    "triplet_selector = SemihardNegativeTripletSelector(2.5, mock_df)\n",
    "indicies = np.array([0,1,2,3,4])\n",
    "embeddings = torch.tensor([[5.,5.],[5.,4.], [5.,3.],[7.,6.], [4.,2.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative triplet selector works!\n"
     ]
    }
   ],
   "source": [
    "output = triplet_selector.get_triplets(embeddings, indicies)\n",
    "expected_output = np.array([[1,0,4],[0,2,3]])\n",
    "np.testing.assert_array_equal(output, expected_output)\n",
    "print('negative triplet selector works!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucket hash tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {(1, 0, 1): [0], (1, 1, 0): [1], (0, 0, 0): [2]})\n",
      "All good!\n"
     ]
    }
   ],
   "source": [
    "from recall import BucketHash\n",
    "buckets = np.array([[1,0,1], [1,1,0],[0,0,0]])\n",
    "bucket_hash = BucketHash()\n",
    "for i,x in enumerate(buckets):\n",
    "    bucket_hash.add(x,i)\n",
    "expected_output = {(1,0,1): [0], (1,1,0): [1], (0,0,0): [2]}\n",
    "print(bucket_hash.hash)\n",
    "for k, val in bucket_hash.hash.items():\n",
    "    np.testing.assert_equal(val, expected_output[k])\n",
    "for i in range(0,len(buckets)):\n",
    "    np.testing.assert_equal(bucket_hash.get(buckets[i]), [i])\n",
    "np.testing.assert_equal(bucket_hash.keys(), buckets)\n",
    "print('All good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_to_proj_map works!\n"
     ]
    }
   ],
   "source": [
    "from recall import Recall\n",
    "r = Recall(5,3)\n",
    "query = np.array([0.6,0.2,-0.4])\n",
    "sorted_proj, mapping = r.sorted_to_proj_map(query)\n",
    "assert(mapping == {0:1, 1:2, 2:0})\n",
    "np.testing.assert_equal(sorted_proj,[0.2, 0.4, 0.6])\n",
    "print(\"sorted_to_proj_map works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done embedding data\n",
      "done quantizing into 4 buckets\n",
      "done embedding data\n",
      "done quantizing into 4 buckets\n",
      "Recall works!\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([\n",
    "    [-1.,2.,3.],\n",
    "    [3.,-4.,-1.],\n",
    "    [-6.,-2.,3.],\n",
    "    [4.,1.,-8.],\n",
    "    [1.,-1.,-1.],\n",
    "    [-1.,2.,.3],\n",
    "    [-2.,1.,4.],\n",
    "    [0.,0.,0.],\n",
    "    [5.,5.,5.],\n",
    "])\n",
    "model = lambda x: torch.mm(x,torch.tensor([[1.,1.], [2.,-1], [0,1.]]))\n",
    "query = torch.tensor([[3.,3.,3.], [3.,3.,3.], [3.,3.,3.]]) #tests same query multiple times\n",
    "mock_true_knns = pd.DataFrame(columns=list(range(0,3)))\n",
    "#test that recall gets all true KNNs %\n",
    "mock_true_knns.loc[0] = [8,0,5] #100% \n",
    "mock_true_knns.loc[1] = [8,0,4] #66%\n",
    "mock_true_knns.loc[2] = [1,2,3] #0%\n",
    "naive = r.calculate(data, model, query, mock_true_knns) #naive slow method\n",
    "generation = r.calculate(data, model, query, mock_true_knns, False) #faster generate to probe method\n",
    "for results in [naive, generation]:\n",
    "    np.testing.assert_equal(results, [1.0, 2/3, 0.])\n",
    "print('Recall works!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis] *",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks the KNN's rankings of datapoints is correct. i.e. anchor-i dist < anchor-j dist for i < j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    return (x-y).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good!\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.from_numpy(np.loadtxt('data/trainData.txt', dtype=np.float32))\n",
    "train_KNN = pd.read_csv('data/trainKNN.csv', index_col=0)\n",
    "# check num elems are same in DF and dataset\n",
    "assert(train_data.shape[0] == train_KNN.shape[0])\n",
    "# #randomly pick 10 rows to check\n",
    "selected_indicies = np.random.choice(list(range(0,train_KNN.shape[0])), size=100, replace=False)\n",
    "# ensure that the ordering of neighbours is correct for that row\n",
    "for index in selected_indicies:\n",
    "    row = train_KNN.iloc[index]\n",
    "    anchor = train_data[index]\n",
    "    for prev, cur in zip(row, row[1:]):\n",
    "        assert((np.linalg.norm(anchor-train_data[prev]) - np.linalg.norm(anchor-train_data[cur])) <= 0)\n",
    "print(\"All Good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generates triplets and ensures that the anchor-pos distance < anchor-neg distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n",
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n",
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n",
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n",
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n",
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n",
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n",
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n",
      "index 38857, pos 44064, neg 14605\n",
      "All Good!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/University/2019/Thesis/code/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train, K, MAX_CLOSE_NEG, MAX_FAR_NEG, P_STRONG_NEG)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/trainData.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/trainKNN.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;31m# converting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'0x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datasets import TripletAudio\n",
    "while True:\n",
    "    dataset = TripletAudio(True, 5, 1, 0, 1)\n",
    "    for i, ((anchor, pos, neg), label, index) in enumerate(dataset):\n",
    "        assert((np.linalg.norm(anchor-pos) - np.linalg.norm(anchor-neg)) <= 0)\n",
    "    print(\"All Good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runs two checks for the embedding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good!\n"
     ]
    }
   ],
   "source": [
    "from networks import AnchorNet\n",
    "#test batch size 1\n",
    "train_data = torch.tensor([[[2.],[3.]]])\n",
    "anchor = AnchorNet([], 2, 1, True)\n",
    "expected_output = torch.tensor([[5**(1/2) - 1]])\n",
    "output = anchor.forward(train_data)\n",
    "assert(torch.equal(output, expected_output))\n",
    "# test batch size 2\n",
    "train_data = torch.tensor([[[12.],[8.]],[[6.],[9.]]])\n",
    "anchor = AnchorNet(train_data, 2, 1, True)\n",
    "expected_output = torch.tensor([[170**(1/2) - 1], [89**(1/2) - 1]])\n",
    "output = anchor.forward(train_data)\n",
    "assert(torch.equal(output, expected_output))\n",
    "print(\"All Good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks similarity generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import similarity_calculator \n",
    "similarity = similarity_calculator(train_KNN, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indicies = np.random.choice(list(range(0,train_KNN.shape[0])), size=100, replace=False)\n",
    "for index in selected_indicies:\n",
    "    similarity_indicies = np.where(similarity[index])[0] #indicies from similarity matrix\n",
    "    expected_indicies = np.append(train_KNN.iloc[index][:5], index) #get true KNN's and add index itself\n",
    "    np.testing.assert_array_equal(np.sort(similarity_indicies), np.sort(expected_indicies))\n",
    "print(\"All Good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks the negative triplet selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mock df to use as the knn dataset\n",
    "mock_df = pd.DataFrame(columns=list(range(0,5)))\n",
    "mock_df.loc[0] = [1,2,5,6,7] #reciprocal neighbours with 1,2. One way with 3\n",
    "mock_df.loc[1] = [0,6,7,8,9] #reciprocal neighbours with 0\n",
    "mock_df.loc[2] = [0,10,11,12,13] #reciprocal neighbours with 0\n",
    "mock_df.loc[3] = [0,15,16,17,18] #one way with 0\n",
    "mock_df.loc[4] = [18,19,20,21,22] #no NN\n",
    "# need to mock remaining for similaritycalc\n",
    "for i in range(5,23):\n",
    "    mock_df.loc[i] = [18,19,20,21,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating similarity matrix\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from utils import SemihardNegativeTripletSelector\n",
    "triplet_selector = SemihardNegativeTripletSelector(2.5, mock_df)\n",
    "indicies = np.array([0,1,2,3,4])\n",
    "embeddings = torch.tensor([[5.,5.],[5.,4.], [5.,3.],[7.,6.], [4.,2.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative triplet selector works!\n"
     ]
    }
   ],
   "source": [
    "output = triplet_selector.get_triplets(embeddings, indicies)\n",
    "expected_output = np.array([[1,0,4],[0,2,3]])\n",
    "np.testing.assert_array_equal(output, expected_output)\n",
    "print('negative triplet selector works!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucket hash tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {(1, 0, 1): [0], (1, 1, 0): [1], (0, 0, 0): [2]})\n",
      "All good!\n"
     ]
    }
   ],
   "source": [
    "from recall import BucketHash\n",
    "buckets = np.array([[1,0,1], [1,1,0],[0,0,0]])\n",
    "bucket_hash = BucketHash()\n",
    "for i,x in enumerate(buckets):\n",
    "    bucket_hash.add(x,i)\n",
    "expected_output = {(1,0,1): [0], (1,1,0): [1], (0,0,0): [2]}\n",
    "print(bucket_hash.hash)\n",
    "for k, val in bucket_hash.hash.items():\n",
    "    np.testing.assert_equal(val, expected_output[k])\n",
    "for i in range(0,len(buckets)):\n",
    "    np.testing.assert_equal(bucket_hash.get(buckets[i]), [i])\n",
    "np.testing.assert_equal(bucket_hash.keys(), buckets)\n",
    "print('All good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_to_proj_map works!\n"
     ]
    }
   ],
   "source": [
    "from recall import Recall\n",
    "r = Recall(5,3)\n",
    "query = np.array([0.6,0.2,-0.4])\n",
    "sorted_proj, mapping = r.sorted_to_proj_map(query)\n",
    "assert(mapping == {0:1, 1:2, 2:0})\n",
    "np.testing.assert_equal(sorted_proj,[0.2, 0.4, 0.6])\n",
    "print(\"sorted_to_proj_map works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done embedding data\n",
      "done quantizing into 4 buckets\n",
      "done embedding data\n",
      "done quantizing into 4 buckets\n",
      "Recall works!\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([\n",
    "    [-1.,2.,3.],\n",
    "    [3.,-4.,-1.],\n",
    "    [-6.,-2.,3.],\n",
    "    [4.,1.,-8.],\n",
    "    [1.,-1.,-1.],\n",
    "    [-1.,2.,.3],\n",
    "    [-2.,1.,4.],\n",
    "    [0.,0.,0.],\n",
    "    [5.,5.,5.],\n",
    "])\n",
    "model = lambda x: torch.mm(x,torch.tensor([[1.,1.], [2.,-1], [0,1.]]))\n",
    "query = torch.tensor([[3.,3.,3.], [3.,3.,3.], [3.,3.,3.]]) #tests same query multiple times\n",
    "mock_true_knns = pd.DataFrame(columns=list(range(0,3)))\n",
    "#test that recall gets all true KNNs %\n",
    "mock_true_knns.loc[0] = [8,0,5] #100% \n",
    "mock_true_knns.loc[1] = [8,0,4] #66%\n",
    "mock_true_knns.loc[2] = [1,2,3] #0%\n",
    "naive = r.calculate(data, model, query, mock_true_knns) #naive slow method\n",
    "generation = r.calculate(data, model, query, mock_true_knns, False) #faster generate to probe method\n",
    "for results in [naive, generation]:\n",
    "    np.testing.assert_equal(results, [1.0, 2/3, 0.])\n",
    "print('Recall works!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis] *",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
